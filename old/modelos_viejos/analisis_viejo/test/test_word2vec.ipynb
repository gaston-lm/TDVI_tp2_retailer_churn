{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import sklearn\n",
    "import re\n",
    "import nltk\n",
    "from hyperopt import hp, tpe, fmin\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score, roc_auc_score, make_scorer\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "\n",
    "seed = 798589991"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_data = pd.read_csv(\"data/eng_data.csv\")\n",
    "all_data = pd.read_csv(\"data/competition_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_data.drop('accepts_mercadopago', inplace=True, axis=1)\n",
    "all_data.drop('accepts_mercadopago', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = all_data['title']\n",
    "comp_data = pd.concat([comp_data, title], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_data = comp_data.loc[:,~(comp_data.columns.str.startswith(\"type_product\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(raw_text):\n",
    "    \"\"\"\n",
    "    Tokeniza y preprocesa un texto.\n",
    "\n",
    "    Args:\n",
    "        raw_text (str): Texto sin procesar.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de oraciones, donde cada oración es una lista de palabras.\n",
    "    \"\"\"\n",
    "    sentences = sent_tokenize(raw_text)\n",
    "    sentences = [word_tokenize(e) for e in sentences]\n",
    "    sentences = [[e2 for e2 in e1 if re.compile(\"[A-Za-z]\").search(e2[0])] for e1 in sentences]\n",
    "    sentences = [[e2.lower() for e2 in e1] for e1 in sentences]\n",
    "    return(sentences)\n",
    "\n",
    "def average_vectors(title_tokens, model, stopwords=None):\n",
    "    \"\"\"\n",
    "    Calcula el vector promedio de un conjunto de tokens utilizando un modelo Word2Vec.\n",
    "\n",
    "    Args:\n",
    "        title_tokens (list): Lista de tokens.\n",
    "        model (gensim.models.Word2Vec): Modelo Word2Vec.\n",
    "        stopwords (set, optional): Conjunto de palabras stopwords. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Vector promedio.\n",
    "    \"\"\"\n",
    "    title_tokens = [e2 for e1 in title_tokens for e2 in e1]\n",
    "    title_tokens = [e for e in title_tokens if e in model.wv]\n",
    "    if stopwords is not None:\n",
    "        title_tokens = [e for e in title_tokens if e not in stopwords]\n",
    "    if len(title_tokens) == 0:\n",
    "        output = np.zeros(model.wv.vector_size)\n",
    "    else:\n",
    "        output = np.array([model.wv.get_vector(e) for e in title_tokens]).mean(0)\n",
    "    return output\n",
    "\n",
    "comp_data[\"title_tokens\"] = comp_data[\"title\"].map(tokenizer)\n",
    "\n",
    "STOP_WORDS_SP = set(stopwords.words('spanish'))\n",
    "\n",
    "# Creación del modelo Word2Vec\n",
    "w2v_tp = gensim.models.Word2Vec(vector_size=300,\n",
    "                                window=3,\n",
    "                                min_count=5,\n",
    "                                negative=15,\n",
    "                                sample=0.01,\n",
    "                                workers=8,\n",
    "                                sg=1)\n",
    "\n",
    "# Creación del vocabulario a partir del corpus\n",
    "w2v_tp.build_vocab([e2 for e1 in comp_data[\"title_tokens\"].values for e2 in e1],\n",
    "                   progress_per=10000)\n",
    "\n",
    "# Entrenamiento del modelo Word2Vec\n",
    "w2v_tp.train([e2 for e1 in comp_data[\"title_tokens\"].values for e2 in e1],\n",
    "             total_examples=w2v_tp.corpus_count,\n",
    "             epochs=30, report_delay=1)\n",
    "\n",
    "title_embs = comp_data[\"title_tokens\"].map(lambda x: average_vectors(x, w2v_tp, STOP_WORDS_SP))\n",
    "embedings = title_embs.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_data['embedings'] = embedings\n",
    "comp_data.drop('title', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividimos entre la data que tenemos y la de evaluación para submitear.\n",
    "\n",
    "# La información que tenemos para entrenar y validar.\n",
    "local_data = comp_data[comp_data[\"ROW_ID\"].isna()]\n",
    "\n",
    "# La información en la que no tenemos las y, para predecir con el modelo ya entrenado y subir a Kaggle.\n",
    "kaggle_data = comp_data[comp_data[\"ROW_ID\"].notna()] \n",
    "\n",
    "# Entrenamos un modelo de xgboost\n",
    "y = local_data[['conversion']].copy()\n",
    "X = local_data.drop(columns=['conversion', 'ROW_ID'], axis = 1)\n",
    "\n",
    "val_test_size = 0.3 # Proporción de la suma del test de validación y del de test.\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = val_test_size, random_state = seed, stratify = y)\n",
    "\n",
    "best_max_depth = 6\n",
    "best_learning_rate = 0.09512855850107411\n",
    "best_n_estimators = 224\n",
    "best_gamma = 0.10113552100045467\n",
    "best_subsample = 0.9158989170972806\n",
    "best_min_child_weight = 3\n",
    "best_colsample_bytree = 0.9000335771350197\n",
    "best_reg_lambda = 12.654060098206006\n",
    "\n",
    "final_cls = make_pipeline(StandardScaler(), SimpleImputer(), xgb.XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        seed=seed,\n",
    "        eval_metric='auc',\n",
    "        max_depth=best_max_depth,\n",
    "        learning_rate=best_learning_rate,\n",
    "        n_estimators=best_n_estimators,\n",
    "        gamma=best_gamma,\n",
    "        subsample=best_subsample,\n",
    "        min_child_weight=best_min_child_weight,\n",
    "        colsample_bytree=best_colsample_bytree,\n",
    "        reg_lambda=best_reg_lambda,\n",
    "    )\n",
    ")\n",
    "\n",
    "final_cls.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Chequeamos el valor debajo de la curva AUC-ROC\n",
    "y_pred = final_cls.predict_proba(X_val)[:, 1]\n",
    "auc_roc = sklearn.metrics.roc_auc_score(y_val, y_pred)\n",
    "print('AUC-ROC validación: %0.5f' % auc_roc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "td6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
