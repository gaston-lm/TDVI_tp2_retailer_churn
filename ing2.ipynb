{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "seed = 798589991"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_data = pd.read_csv(\"data/competition_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['accepts_mercadopago', 'available_quantity',\n",
       "       'avg_gmv_item_domain_30days', 'avg_gmv_item_sel', 'avg_gmv_seller_bday',\n",
       "       'avg_qty_orders_item_domain_30days', 'avg_qty_orders_item_sel_30days',\n",
       "       'avg_si_item_sel_30day', 'benefit', 'boosted', 'category_id',\n",
       "       'conversion', 'date', 'deal_print_id', 'domain_id', 'etl_version',\n",
       "       'free_shipping', 'fulfillment', 'full_name', 'health', 'is_pdp',\n",
       "       'product_id', 'item_id', 'listing_type_id', 'logistic_type',\n",
       "       'main_picture', 'offset', 'original_price', 'platform', 'price',\n",
       "       'print_position', 'print_server_timestamp', 'qty_items_dom',\n",
       "       'qty_items_sel', 'site_id', 'sold_quantity', 'tags', 'title',\n",
       "       'total_asp_item_domain_30days', 'total_asp_item_sel_30days',\n",
       "       'total_gmv_domain_bday', 'total_gmv_item_30days', 'total_items_domain',\n",
       "       'total_items_seller', 'total_orders_domain_30days',\n",
       "       'total_orders_item_30days', 'total_orders_sel_30days',\n",
       "       'total_si_domain_30days', 'total_si_item_30days', 'total_si_sel_30days',\n",
       "       'total_visits_domain', 'total_visits_item', 'total_visits_seller',\n",
       "       'uid', 'user_id', 'warranty', 'ROW_ID'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>print_server_timestamp</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-03-06T00:19:30.735-0400</td>\n",
       "      <td>2020-03-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-04-01T21:20:11.738-0400</td>\n",
       "      <td>2020-04-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-04-08T18:38:48.360-0400</td>\n",
       "      <td>2020-04-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-04-25T22:01:19.829-0400</td>\n",
       "      <td>2020-04-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-03-10T13:20:56.633-0400</td>\n",
       "      <td>2020-03-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199967</th>\n",
       "      <td>2020-04-12T14:35:44.784-0400</td>\n",
       "      <td>2020-04-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199968</th>\n",
       "      <td>2020-04-03T17:32:53.035-0400</td>\n",
       "      <td>2020-04-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199969</th>\n",
       "      <td>2020-04-01T22:51:08.898-0400</td>\n",
       "      <td>2020-04-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199970</th>\n",
       "      <td>2020-04-08T11:32:27.563-0400</td>\n",
       "      <td>2020-04-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199971</th>\n",
       "      <td>2020-03-22T20:32:01.677-0400</td>\n",
       "      <td>2020-03-22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>199972 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              print_server_timestamp        date\n",
       "0       2020-03-06T00:19:30.735-0400  2020-03-06\n",
       "1       2020-04-01T21:20:11.738-0400  2020-04-01\n",
       "2       2020-04-08T18:38:48.360-0400  2020-04-08\n",
       "3       2020-04-25T22:01:19.829-0400  2020-04-25\n",
       "4       2020-03-10T13:20:56.633-0400  2020-03-10\n",
       "...                              ...         ...\n",
       "199967  2020-04-12T14:35:44.784-0400  2020-04-12\n",
       "199968  2020-04-03T17:32:53.035-0400  2020-04-03\n",
       "199969  2020-04-01T22:51:08.898-0400  2020-04-01\n",
       "199970  2020-04-08T11:32:27.563-0400  2020-04-08\n",
       "199971  2020-03-22T20:32:01.677-0400  2020-03-22\n",
       "\n",
       "[199972 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_data[['print_server_timestamp', 'date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "android    124781\n",
       "desktop     35892\n",
       "mobile      25092\n",
       "ios         14207\n",
       "Name: platform, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_data['platform'].str.split('/').str[2].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    176190\n",
       "True      23782\n",
       "Name: warranty, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_data['warranty'].str.split().str[0].str.upper().eq('SIN').value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts_ult = comp_data['full_name'].str.split(' -> ').str[-1].value_counts()\n",
    "# counts_pri = comp_data['full_name'].str.split(' -> ').str[0].value_counts()\n",
    "# tal vez dejar la más amplia como OHE y la otra hacer algo de word2vec?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropeo las columnas.\n",
    "comp_data.drop('benefit', inplace=True, axis=1)\n",
    "comp_data.drop('user_id', inplace=True, axis=1)\n",
    "comp_data.drop('uid', inplace=True, axis=1)\n",
    "comp_data.drop('main_picture', inplace=True, axis=1)\n",
    "comp_data.drop('category_id', inplace=True, axis=1)\n",
    "comp_data.drop('domain_id', inplace=True, axis=1)\n",
    "comp_data.drop('deal_print_id', inplace=True, axis=1)\n",
    "comp_data.drop('etl_version', inplace=True, axis=1)\n",
    "comp_data.drop('product_id', inplace=True, axis=1)\n",
    "# comp_data.drop('title', inplace=True, axis=1) # habría que usarlo con word2vec?\n",
    "comp_data.drop('site_id', inplace=True, axis=1)\n",
    "comp_data.drop('item_id', inplace=True, axis=1)\n",
    "comp_data.drop('date', inplace=True, axis=1)\n",
    "comp_data.drop('accepts_mercadopago', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diferenciar desktop, ios, android o mobile(/web/mobile debe ser desde navegador y /mobile/ios desde app)\n",
    "comp_data['platform'] = comp_data['platform'].str.split('/').str[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener primera y última categoría\n",
    "comp_data['category_first'] = comp_data['full_name'].str.split(' -> ').str[0]\n",
    "comp_data['category_last'] = comp_data['full_name'].str.split(' -> ').str[-1]\n",
    "\n",
    "comp_data.drop('full_name', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformo garantía en una columna binaria (True, False, NaN)\n",
    "comp_data['warranty'] = (\n",
    "    ~comp_data['warranty'].str.split().str[0].str.upper().eq('SIN')\n",
    ").where(comp_data['warranty'].notna()).astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo una columna con el descuento (en porcentaje).\n",
    "discount = (((comp_data['original_price'] - comp_data['price']) / comp_data['original_price']) * 100).astype(int)\n",
    "comp_data['discount_%'] = discount\n",
    "\n",
    "comp_data.drop('original_price', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consigo los tags posibles.\n",
    "unique_tags = []\n",
    "for list in comp_data['tags']:\n",
    "    list_split = list[1:len(list)-1].split(', ')\n",
    "    for item in list_split:\n",
    "        if not (item in unique_tags):\n",
    "            unique_tags.append(item)\n",
    "\n",
    "# Separo los tags en columnas de booleanos.\n",
    "for tag in unique_tags:\n",
    "    comp_data[tag] = comp_data['tags'].apply(lambda x: tag in x)\n",
    "\n",
    "comp_data.drop('tags', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacer algo inteligente con la date.\n",
    "comp_data['date'] = pd.to_datetime(comp_data['print_server_timestamp'])\n",
    "comp_data['month'] = comp_data['date'].dt.month\n",
    "comp_data['day'] = comp_data['date'].dt.day\n",
    "comp_data['day_of_week'] = comp_data['date'].dt.dayofweek\n",
    "comp_data['hour'] = comp_data['date'].dt.hour\n",
    "comp_data['minute'] = comp_data['date'].dt.minute\n",
    "comp_data['second'] = comp_data['date'].dt.second\n",
    "\n",
    "comp_data.drop('date', inplace=True, axis=1)\n",
    "comp_data.drop('print_server_timestamp', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(raw_text):\n",
    "    \"\"\"\n",
    "    Tokeniza y preprocesa un texto.\n",
    "\n",
    "    Args:\n",
    "        raw_text (str): Texto sin procesar.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de oraciones, donde cada oración es una lista de palabras.\n",
    "    \"\"\"\n",
    "    sentences = sent_tokenize(raw_text)\n",
    "    sentences = [word_tokenize(e) for e in sentences]\n",
    "    sentences = [[e2 for e2 in e1 if re.compile(\"[A-Za-z]\").search(e2[0])] for e1 in sentences]\n",
    "    sentences = [[e2.lower() for e2 in e1] for e1 in sentences]\n",
    "    return(sentences)\n",
    "\n",
    "def average_vectors(title_tokens, model, stopwords=None):\n",
    "    \"\"\"\n",
    "    Calcula el vector promedio de un conjunto de tokens utilizando un modelo Word2Vec.\n",
    "\n",
    "    Args:\n",
    "        title_tokens (list): Lista de tokens.\n",
    "        model (gensim.models.Word2Vec): Modelo Word2Vec.\n",
    "        stopwords (set, optional): Conjunto de palabras stopwords. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Vector promedio.\n",
    "    \"\"\"\n",
    "    title_tokens = [e2 for e1 in title_tokens for e2 in e1]\n",
    "    title_tokens = [e for e in title_tokens if e in model.wv]\n",
    "    if stopwords is not None:\n",
    "        title_tokens = [e for e in title_tokens if e not in stopwords]\n",
    "    if len(title_tokens) == 0:\n",
    "        output = np.zeros(model.wv.vector_size)\n",
    "    else:\n",
    "        output = np.array([model.wv.get_vector(e) for e in title_tokens]).mean(0)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Para title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gaston\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "comp_data['title_tokens'] = comp_data['title'].map(tokenizer)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "STOP_WORDS_SP = set(stopwords.words('spanish'))\n",
    "\n",
    "# Creación del modelo Word2Vec\n",
    "w2v_title = gensim.models.Word2Vec(vector_size=300,\n",
    "                                   window=3,\n",
    "                                   min_count=5,\n",
    "                                   negative=15,\n",
    "                                   sample=0.01,\n",
    "                                   workers=8,\n",
    "                                   sg=1)\n",
    "\n",
    "# Creación del vocabulario a partir del corpus\n",
    "w2v_title.build_vocab([e2 for e1 in comp_data['title_tokens'].values for e2 in e1], \n",
    "                       progress_per=10000)\n",
    "\n",
    "# Entrenamiento del modelo Word2Vec\n",
    "w2v_title.train([e2 for e1 in comp_data['title_tokens'].values for e2 in e1],\n",
    "                total_examples=w2v_title.corpus_count,\n",
    "                epochs=30, report_delay=1)\n",
    "\n",
    "title_embs = comp_data['title_tokens'].map(lambda x: average_vectors(x, w2v_title, STOP_WORDS_SP))\n",
    "embedding_title_columns = pd.DataFrame(title_embs.tolist(), columns=[f'title_emb_{i}' for i in range(300)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Para category last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gaston\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "comp_data['category_last_tokens'] = comp_data['category_last'].map(tokenizer)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "STOP_WORDS_SP = set(stopwords.words('spanish'))\n",
    "\n",
    "# Creación del modelo Word2Vec\n",
    "w2v_category = gensim.models.Word2Vec(vector_size=300,\n",
    "                                      window=3,\n",
    "                                      min_count=5,\n",
    "                                      negative=15,\n",
    "                                      sample=0.01,\n",
    "                                      workers=8,\n",
    "                                      sg=1)\n",
    "\n",
    "# Creación del vocabulario a partir del corpus\n",
    "w2v_category.build_vocab([e2 for e1 in comp_data['category_last_tokens'].values for e2 in e1], \n",
    "                          progress_per=10000)\n",
    "\n",
    "# Entrenamiento del modelo Word2Vec\n",
    "w2v_category.train([e2 for e1 in comp_data['category_last_tokens'].values for e2 in e1],\n",
    "                    total_examples=w2v_title.corpus_count,\n",
    "                    epochs=30, report_delay=1)\n",
    "\n",
    "category_embs = comp_data['category_last_tokens'].map(lambda x: average_vectors(x, w2v_category, STOP_WORDS_SP))\n",
    "embedding_category_columns = pd.DataFrame(title_embs.tolist(), columns=[f'category_emb_{i}' for i in range(300)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_data.drop('title_tokens', inplace=True, axis=1)\n",
    "comp_data.drop('category_last_tokens', inplace=True, axis=1)\n",
    "comp_data.drop('title', inplace=True, axis=1)\n",
    "comp_data.drop('category_last', inplace=True, axis=1)\n",
    "comp_data = pd.concat([comp_data, embedding_title_columns], axis=1)\n",
    "comp_data = pd.concat([comp_data, embedding_category_columns], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_encode = ['category_first', 'listing_type_id', 'logistic_type', 'platform']\n",
    "comp_data_encoded = pd.get_dummies(comp_data[cols_to_encode])\n",
    "comp_data = pd.concat([comp_data, comp_data_encoded], axis=1)\n",
    "comp_data.drop(columns=cols_to_encode, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Antes de empezar el entrenamiento del modelo, paso a int las columnas de booleano. Lo hago así para no tener problemas con los NaNs\n",
    "comp_data.replace({False: 0, True: 1}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_data.to_csv(\"data/2609_data_w2v.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "td6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
