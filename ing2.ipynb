{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "seed = 798589991"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_data = pd.read_csv(\"data/competition_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['accepts_mercadopago', 'available_quantity',\n",
       "       'avg_gmv_item_domain_30days', 'avg_gmv_item_sel', 'avg_gmv_seller_bday',\n",
       "       'avg_qty_orders_item_domain_30days', 'avg_qty_orders_item_sel_30days',\n",
       "       'avg_si_item_sel_30day', 'benefit', 'boosted', 'category_id',\n",
       "       'conversion', 'date', 'deal_print_id', 'domain_id', 'etl_version',\n",
       "       'free_shipping', 'fulfillment', 'full_name', 'health', 'is_pdp',\n",
       "       'product_id', 'item_id', 'listing_type_id', 'logistic_type',\n",
       "       'main_picture', 'offset', 'original_price', 'platform', 'price',\n",
       "       'print_position', 'print_server_timestamp', 'qty_items_dom',\n",
       "       'qty_items_sel', 'site_id', 'sold_quantity', 'tags', 'title',\n",
       "       'total_asp_item_domain_30days', 'total_asp_item_sel_30days',\n",
       "       'total_gmv_domain_bday', 'total_gmv_item_30days', 'total_items_domain',\n",
       "       'total_items_seller', 'total_orders_domain_30days',\n",
       "       'total_orders_item_30days', 'total_orders_sel_30days',\n",
       "       'total_si_domain_30days', 'total_si_item_30days', 'total_si_sel_30days',\n",
       "       'total_visits_domain', 'total_visits_item', 'total_visits_seller',\n",
       "       'uid', 'user_id', 'warranty', 'ROW_ID'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Para Garantía"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "warranty_types = comp_data['warranty']\n",
    "warranty_types = warranty_types.reset_index()\n",
    "warranty_types['Warranty Type Words'] = warranty_types['warranty'].apply(lambda x: str(x).upper().split() if not pd.isna(x) else [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the words you want to check for\n",
    "words = ['SIN','GARANTÍA','GARANTIA','VENDEDOR:','VENDEDORES','VENDEDOR','VENDEDOR,','FÁBRICA:','FABRICA','FÁBRICA','FÁBRICA.','FÁBRICA,','MESES','MESES.','DÍAS','DIAS','DÍAS,','DÍAS.','AÑOS','AÑOS.','ANO','AÑO','AÑO.']\n",
    "\n",
    "# Check if each word exists in each warranty type\n",
    "for word in words:\n",
    "    warranty_types[word] = warranty_types['Warranty Type Words'].apply(lambda x: word in x)\n",
    "\n",
    "warranty_types['Numbers'] = warranty_types['warranty'].str.extract(r'(\\d+)')\n",
    "\n",
    "warranty_types['is_warranty_fábrica'] = warranty_types['FÁBRICA:'] | warranty_types['FÁBRICA'] | warranty_types['FABRICA'] | warranty_types['FÁBRICA.'] | warranty_types['FÁBRICA,']\n",
    "warranty_types['is_warranty_vendedor'] = warranty_types['VENDEDOR:'] | warranty_types['VENDEDOR'] | warranty_types['VENDEDORES'] | warranty_types['VENDEDOR,']\n",
    "warranty_types['time_dias'] = warranty_types['DÍAS'] | warranty_types['DIAS'] | warranty_types['DÍAS,'] | warranty_types['DÍAS.']  \n",
    "warranty_types['time_meses'] = warranty_types['MESES'] | warranty_types['MESES.'] \n",
    "warranty_types['time_años'] = warranty_types['AÑOS'] | warranty_types['AÑOS.'] | warranty_types['AÑO'] | warranty_types['ANO'] | warranty_types['AÑO.']\n",
    "warranty_types['has_warranty'] = ~(((warranty_types['SIN'] & warranty_types['GARANTÍA']) | (warranty_types['SIN'] & warranty_types['GARANTIA'])) & warranty_types['Numbers'].isna())\n",
    "\n",
    "warranty_types.drop(columns=words, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 'time' column based on conditions\n",
    "conditions = [\n",
    "    warranty_types['time_meses'] & warranty_types['time_dias'],  # 'meses' \n",
    "    warranty_types['time_años'] & warranty_types['time_meses'],   # 'meses'\n",
    "    warranty_types['time_años'],  # 'años'\n",
    "    warranty_types['time_meses'],  # 'meses'\n",
    "    warranty_types['time_dias']  # 'días'\n",
    "]\n",
    "\n",
    "choices = ['meses', 'meses', 'años', 'meses', 'días']\n",
    "\n",
    "warranty_types['time'] = np.select(conditions, choices, default=np.nan)\n",
    "warranty_types.drop(columns=['time_meses', 'time_dias', 'time_años'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 'type_warranty' column based on conditions\n",
    "conditions = [\n",
    "    warranty_types['is_warranty_fábrica'],  # 'fábrica' \n",
    "    warranty_types['is_warranty_vendedor'],   # 'vendedor'\n",
    "]\n",
    "\n",
    "choices = ['fábrica', 'vendedor']\n",
    "\n",
    "warranty_types['type_warranty'] = np.select(conditions, choices, default=np.nan)\n",
    "warranty_types.drop(columns=['is_warranty_fábrica', 'is_warranty_vendedor'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "warranty_types['warranty_time'] = np.where(warranty_types['Numbers'].isna() | warranty_types['time'].isna(), np.nan,\n",
    "                                           'warranty_time_' + warranty_types['Numbers'].astype(str) + '_' + warranty_types['time'].astype(str))\n",
    "\n",
    "warranty_types.drop(columns=['Numbers', 'time'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "warranty_types_encoded = pd.get_dummies(warranty_types['type_warranty'])\n",
    "new_column_names = ['type_warranty_fabrica', 'type_warranty_nan', 'type_warranty_vendedor']\n",
    "warranty_types_encoded.columns = new_column_names\n",
    "warranty_types = pd.concat([warranty_types, warranty_types_encoded], axis=1)\n",
    "warranty_types.drop(columns=['type_warranty', 'Warranty Type Words'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "warranty_types['warranty_time'] = warranty_types['warranty_time'].fillna('warranty_time_nan')\n",
    "warranty_types_encoded = pd.get_dummies(warranty_types['warranty_time'])\n",
    "warranty_types = pd.concat([warranty_types, warranty_types_encoded], axis=1)\n",
    "warranty_types.drop(columns=['warranty_time', 'warranty'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_data = pd.concat([comp_data, warranty_types], axis=1)\n",
    "comp_data.drop(columns=['warranty'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>print_server_timestamp</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-03-06T00:19:30.735-0400</td>\n",
       "      <td>2020-03-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-04-01T21:20:11.738-0400</td>\n",
       "      <td>2020-04-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-04-08T18:38:48.360-0400</td>\n",
       "      <td>2020-04-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-04-25T22:01:19.829-0400</td>\n",
       "      <td>2020-04-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-03-10T13:20:56.633-0400</td>\n",
       "      <td>2020-03-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199967</th>\n",
       "      <td>2020-04-12T14:35:44.784-0400</td>\n",
       "      <td>2020-04-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199968</th>\n",
       "      <td>2020-04-03T17:32:53.035-0400</td>\n",
       "      <td>2020-04-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199969</th>\n",
       "      <td>2020-04-01T22:51:08.898-0400</td>\n",
       "      <td>2020-04-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199970</th>\n",
       "      <td>2020-04-08T11:32:27.563-0400</td>\n",
       "      <td>2020-04-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199971</th>\n",
       "      <td>2020-03-22T20:32:01.677-0400</td>\n",
       "      <td>2020-03-22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>199972 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              print_server_timestamp        date\n",
       "0       2020-03-06T00:19:30.735-0400  2020-03-06\n",
       "1       2020-04-01T21:20:11.738-0400  2020-04-01\n",
       "2       2020-04-08T18:38:48.360-0400  2020-04-08\n",
       "3       2020-04-25T22:01:19.829-0400  2020-04-25\n",
       "4       2020-03-10T13:20:56.633-0400  2020-03-10\n",
       "...                              ...         ...\n",
       "199967  2020-04-12T14:35:44.784-0400  2020-04-12\n",
       "199968  2020-04-03T17:32:53.035-0400  2020-04-03\n",
       "199969  2020-04-01T22:51:08.898-0400  2020-04-01\n",
       "199970  2020-04-08T11:32:27.563-0400  2020-04-08\n",
       "199971  2020-03-22T20:32:01.677-0400  2020-03-22\n",
       "\n",
       "[199972 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_data[['print_server_timestamp', 'date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "android    124781\n",
       "desktop     35892\n",
       "mobile      25092\n",
       "ios         14207\n",
       "Name: platform, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_data['platform'].str.split('/').str[2].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    176190\n",
       "True      23782\n",
       "Name: warranty, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_data['warranty'].str.split().str[0].str.upper().eq('SIN').value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts_ult = comp_data['full_name'].str.split(' -> ').str[-1].value_counts()\n",
    "# counts_pri = comp_data['full_name'].str.split(' -> ').str[0].value_counts()\n",
    "# tal vez dejar la más amplia como OHE y la otra hacer algo de word2vec?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropeo las columnas.\n",
    "comp_data.drop('benefit', inplace=True, axis=1)\n",
    "# comp_data.drop('user_id', inplace=True, axis=1)\n",
    "comp_data.drop('uid', inplace=True, axis=1)\n",
    "comp_data.drop('main_picture', inplace=True, axis=1)\n",
    "comp_data.drop('category_id', inplace=True, axis=1)\n",
    "comp_data.drop('domain_id', inplace=True, axis=1)\n",
    "comp_data.drop('deal_print_id', inplace=True, axis=1)\n",
    "comp_data.drop('etl_version', inplace=True, axis=1)\n",
    "# comp_data.drop('product_id', inplace=True, axis=1)\n",
    "comp_data.drop('site_id', inplace=True, axis=1)\n",
    "comp_data.drop('item_id', inplace=True, axis=1)\n",
    "comp_data.drop('date', inplace=True, axis=1)\n",
    "comp_data.drop('accepts_mercadopago', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diferenciar desktop, ios, android o mobile(/web/mobile debe ser desde navegador y /mobile/ios desde app)\n",
    "comp_data['platform'] = comp_data['platform'].str.split('/').str[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener primera y última categoría\n",
    "comp_data['category_first'] = comp_data['full_name'].str.split(' -> ').str[0]\n",
    "comp_data['category_last'] = comp_data['full_name'].str.split(' -> ').str[-1]\n",
    "\n",
    "comp_data.drop('full_name', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformo garantía en una columna binaria (True, False, NaN)\n",
    "comp_data['warranty'] = (\n",
    "    ~comp_data['warranty'].str.split().str[0].str.upper().eq('SIN')\n",
    ").where(comp_data['warranty'].notna()).astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo una columna con el descuento (en porcentaje).\n",
    "discount = (((comp_data['original_price'] - comp_data['price']) / comp_data['original_price']) * 100).astype(int)\n",
    "comp_data['discount_%'] = discount\n",
    "\n",
    "comp_data.drop('original_price', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consigo los tags posibles.\n",
    "unique_tags = []\n",
    "for list in comp_data['tags']:\n",
    "    list_split = list[1:len(list)-1].split(', ')\n",
    "    for item in list_split:\n",
    "        if not (item in unique_tags):\n",
    "            unique_tags.append(item)\n",
    "\n",
    "# Separo los tags en columnas de booleanos.\n",
    "for tag in unique_tags:\n",
    "    comp_data[tag] = comp_data['tags'].apply(lambda x: tag in x)\n",
    "\n",
    "comp_data.drop('tags', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacer algo inteligente con la date.\n",
    "comp_data['date'] = pd.to_datetime(comp_data['print_server_timestamp'])\n",
    "comp_data['month'] = comp_data['date'].dt.month\n",
    "comp_data['day'] = comp_data['date'].dt.day\n",
    "comp_data['day_of_week'] = comp_data['date'].dt.dayofweek\n",
    "comp_data['hour'] = comp_data['date'].dt.hour\n",
    "comp_data['minute'] = comp_data['date'].dt.minute\n",
    "comp_data['second'] = comp_data['date'].dt.second\n",
    "\n",
    "comp_data.drop('date', inplace=True, axis=1)\n",
    "comp_data.drop('print_server_timestamp', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "# comp_data = comp_data.loc[:,~comp_data.columns.str.startswith('warranty')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esto puede generar data leakage pero como al final entrenamos con todo lo pruebo y valido con Kaggle\n",
    "\n",
    "user_counting = comp_data['user_id'].value_counts()\n",
    "\n",
    "uid_ratio = comp_data[['user_id', 'conversion']]\n",
    "pid_ratio = comp_data[['product_id', 'conversion']]\n",
    "\n",
    "# Calculate conversion frequency\n",
    "conversion_frequency = pid_ratio.groupby('product_id')['conversion'].mean().reset_index()\n",
    "\n",
    "# Calculate count of each unique product_id\n",
    "product_id_count = pid_ratio['product_id'].value_counts().reset_index()\n",
    "product_id_count.columns = ['product_id', 'pid_count']\n",
    "\n",
    "# Merge the two DataFrames\n",
    "result = pd.merge(conversion_frequency, product_id_count, on='product_id')\n",
    "\n",
    "# Rename the columns if needed\n",
    "result.rename(columns={'conversion': 'conv_freq_pid'}, inplace=True)\n",
    "\n",
    "# Calculate conversion frequency\n",
    "conversion_frequency_user = uid_ratio.groupby('user_id')['conversion'].mean().reset_index()\n",
    "\n",
    "# Calculate count of each unique product_id\n",
    "user_id_count = uid_ratio['user_id'].value_counts().reset_index()\n",
    "user_id_count.columns = ['user_id', 'uid_count']\n",
    "\n",
    "# Merge the two DataFrames\n",
    "result_uid = pd.merge(conversion_frequency_user, user_id_count, on='user_id')\n",
    "\n",
    "# Rename the columns if needed\n",
    "result_uid.rename(columns={'conversion': 'conv_freq_uid'}, inplace=True)\n",
    "\n",
    "# comp_data = pd.merge(result_uid, comp_data, on='user_id')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(raw_text):\n",
    "    \"\"\"\n",
    "    Tokeniza y preprocesa un texto.\n",
    "\n",
    "    Args:\n",
    "        raw_text (str): Texto sin procesar.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de oraciones, donde cada oración es una lista de palabras.\n",
    "    \"\"\"\n",
    "    sentences = sent_tokenize(raw_text)\n",
    "    sentences = [word_tokenize(e) for e in sentences]\n",
    "    sentences = [[e2 for e2 in e1 if re.compile(\"[A-Za-z]\").search(e2[0])] for e1 in sentences]\n",
    "    sentences = [[e2.lower() for e2 in e1] for e1 in sentences]\n",
    "    return(sentences)\n",
    "\n",
    "def average_vectors(title_tokens, model, stopwords=None):\n",
    "    \"\"\"\n",
    "    Calcula el vector promedio de un conjunto de tokens utilizando un modelo Word2Vec.\n",
    "\n",
    "    Args:\n",
    "        title_tokens (list): Lista de tokens.\n",
    "        model (gensim.models.Word2Vec): Modelo Word2Vec.\n",
    "        stopwords (set, optional): Conjunto de palabras stopwords. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Vector promedio.\n",
    "    \"\"\"\n",
    "    title_tokens = [e2 for e1 in title_tokens for e2 in e1]\n",
    "    title_tokens = [e for e in title_tokens if e in model.wv]\n",
    "    if stopwords is not None:\n",
    "        title_tokens = [e for e in title_tokens if e not in stopwords]\n",
    "    if len(title_tokens) == 0:\n",
    "        output = np.zeros(model.wv.vector_size)\n",
    "    else:\n",
    "        output = np.array([model.wv.get_vector(e) for e in title_tokens]).mean(0)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\feder\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\feder\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Para title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_data['title_tokens'] = comp_data['title'].map(tokenizer)\n",
    "\n",
    "STOP_WORDS_SP = set(stopwords.words('spanish'))\n",
    "\n",
    "# Creación del modelo Word2Vec\n",
    "w2v_title = gensim.models.Word2Vec(vector_size=100,\n",
    "                                   window=3,\n",
    "                                   min_count=5,\n",
    "                                   negative=15,\n",
    "                                   sample=0.01,\n",
    "                                   workers=8,\n",
    "                                   sg=1)\n",
    "\n",
    "# Creación del vocabulario a partir del corpus\n",
    "w2v_title.build_vocab([e2 for e1 in comp_data['title_tokens'].values for e2 in e1], \n",
    "                       progress_per=10000)\n",
    "\n",
    "# Entrenamiento del modelo Word2Vec\n",
    "w2v_title.train([e2 for e1 in comp_data['title_tokens'].values for e2 in e1],\n",
    "                total_examples=w2v_title.corpus_count,\n",
    "                epochs=30, report_delay=1)\n",
    "\n",
    "title_embs = comp_data['title_tokens'].map(lambda x: average_vectors(x, w2v_title, STOP_WORDS_SP))\n",
    "embedding_title_columns = pd.DataFrame(title_embs.tolist(), columns=[f'title_emb_{i}' for i in range(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_data.drop('title_tokens', inplace=True, axis=1)\n",
    "comp_data.drop('title', inplace=True, axis=1)\n",
    "comp_data = pd.concat([comp_data, embedding_title_columns], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_encode = ['category_first', 'listing_type_id', 'logistic_type', 'platform', 'category_last']\n",
    "comp_data_encoded = pd.get_dummies(comp_data[cols_to_encode])\n",
    "comp_data = pd.concat([comp_data, comp_data_encoded], axis=1)\n",
    "comp_data.drop(columns=cols_to_encode, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Antes de empezar el entrenamiento del modelo, paso a int las columnas de booleano. Lo hago así para no tener problemas con los NaNs\n",
    "comp_data.replace({False: 0, True: 1}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_data.to_csv(\"data/new_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC-ROC validación: 0.89293 --> xgboost_model_w2v_2709\n",
    "# AUC-ROC validación: 0.89365 --> xgboost_model_w2v100_war_2709"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "td6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
