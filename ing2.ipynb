{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "seed = 798589991"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_data = pd.read_csv(\"data/competition_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "local_data = comp_data[comp_data[\"ROW_ID\"].isna()]\n",
    "\n",
    "y = local_data[['conversion']].copy()\n",
    "X = local_data.drop(columns=['conversion', 'ROW_ID'], axis = 1)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state = 155, stratify = y)\n",
    "\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "val_data = pd.concat([X_val, y_val], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop('conversion', inplace=True, axis=1)\n",
    "val_data.drop('conversion', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = comp_data.groupby('product_id')['product_id'].transform('count')\n",
    "test = pd.DataFrame({'pid_count': test, 'product_id': comp_data['product_id'], 'conversion': comp_data['conversion']})\n",
    "\n",
    "test['product_conversion_sum'] = test.groupby('product_id')['conversion'].transform('sum')\n",
    "test['conv_frecuency'] = test['product_conversion_sum'] / test['pid_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['product_id'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['conversion'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_data[\"ROW_ID\"].notna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_data = comp_data[comp_data[\"ROW_ID\"].notna()] \n",
    "kaggle_data['product_id'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# title\n",
    "title = comp_data[['title', 'conversion']]\n",
    "title = title.reset_index()\n",
    "title['title_words'] = title['title'].apply(lambda x: str(x).upper().split() if not pd.isna(x) else [])\n",
    "\n",
    "words = ['NUEVO','CUOTAS','OFICIAL', 'ENVIO', 'ENVÍO']\n",
    "\n",
    "for word in words:\n",
    "    title[word] = title['title_words'].apply(lambda x: word in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Nuevo: {title['NUEVO'].sum()}\")\n",
    "print(f\"Cuotas: {title['CUOTAS'].sum()}\")\n",
    "print(f\"Oficial: {title['OFICIAL'].sum()}\")\n",
    "print(f\"Envio: {title['ENVIO'].sum()}\")\n",
    "print(f\"Envío: {title['ENVÍO'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(title[title['OFICIAL']==True]['conversion'].sum() / title['OFICIAL'].sum())\n",
    "print(title[title['NUEVO']==True]['conversion'].sum() / title['NUEVO'].sum())\n",
    "print(title[title['CUOTAS']==True]['conversion'].sum() / title['CUOTAS'].sum())\n",
    "print(title[title['ENVIO']==True]['conversion'].sum() / title['ENVIO'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warranty_types = comp_data['warranty']\n",
    "warranty_types = warranty_types.reset_index()\n",
    "warranty_types['warranty_words'] = warranty_types['warranty'].apply(lambda x: str(x).upper().split() if not pd.isna(x) else [])\n",
    "\n",
    "words = ['SIN','GARANTÍA','GARANTIA']\n",
    "\n",
    "for word in words:\n",
    "    warranty_types[word] = warranty_types['warranty_words'].apply(lambda x: word in x)\n",
    "\n",
    "warranty_types['Numbers'] = warranty_types['warranty'].str.extract(r'(\\d+)')\n",
    "warranty_types['has_warranty'] = ~(((warranty_types['SIN'] & warranty_types['GARANTÍA']) | (warranty_types['SIN'] & warranty_types['GARANTIA'])) & warranty_types['Numbers'].isna())\n",
    "\n",
    "comp_data['warranty'] = warranty_types['has_warranty']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Para Garantía"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warranty_types = comp_data['warranty']\n",
    "# warranty_types = warranty_types.reset_index()\n",
    "# warranty_types['Warranty Type Words'] = warranty_types['warranty'].apply(lambda x: str(x).upper().split() if not pd.isna(x) else [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the words you want to check for\n",
    "# words = ['SIN','GARANTÍA','GARANTIA','VENDEDOR:','VENDEDORES','VENDEDOR','VENDEDOR,','FÁBRICA:','FABRICA','FÁBRICA','FÁBRICA.','FÁBRICA,','MESES','MESES.','DÍAS','DIAS','DÍAS,','DÍAS.','AÑOS','AÑOS.','ANO','AÑO','AÑO.']\n",
    "\n",
    "# # Check if each word exists in each warranty type\n",
    "# for word in words:\n",
    "#     warranty_types[word] = warranty_types['Warranty Type Words'].apply(lambda x: word in x)\n",
    "\n",
    "# warranty_types['Numbers'] = warranty_types['warranty'].str.extract(r'(\\d+)')\n",
    "\n",
    "# warranty_types['is_warranty_fábrica'] = warranty_types['FÁBRICA:'] | warranty_types['FÁBRICA'] | warranty_types['FABRICA'] | warranty_types['FÁBRICA.'] | warranty_types['FÁBRICA,']\n",
    "# warranty_types['is_warranty_vendedor'] = warranty_types['VENDEDOR:'] | warranty_types['VENDEDOR'] | warranty_types['VENDEDORES'] | warranty_types['VENDEDOR,']\n",
    "# warranty_types['time_dias'] = warranty_types['DÍAS'] | warranty_types['DIAS'] | warranty_types['DÍAS,'] | warranty_types['DÍAS.']  \n",
    "# warranty_types['time_meses'] = warranty_types['MESES'] | warranty_types['MESES.'] \n",
    "# warranty_types['time_años'] = warranty_types['AÑOS'] | warranty_types['AÑOS.'] | warranty_types['AÑO'] | warranty_types['ANO'] | warranty_types['AÑO.']\n",
    "# warranty_types['has_warranty'] = ~(((warranty_types['SIN'] & warranty_types['GARANTÍA']) | (warranty_types['SIN'] & warranty_types['GARANTIA'])) & warranty_types['Numbers'].isna())\n",
    "\n",
    "# warranty_types.drop(columns=words, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create the 'time' column based on conditions\n",
    "# conditions = [\n",
    "#     warranty_types['time_meses'] & warranty_types['time_dias'],  # 'meses' \n",
    "#     warranty_types['time_años'] & warranty_types['time_meses'],   # 'meses'\n",
    "#     warranty_types['time_años'],  # 'años'\n",
    "#     warranty_types['time_meses'],  # 'meses'\n",
    "#     warranty_types['time_dias']  # 'días'\n",
    "# ]\n",
    "\n",
    "# choices = ['meses', 'meses', 'años', 'meses', 'días']\n",
    "\n",
    "# warranty_types['time'] = np.select(conditions, choices, default=np.nan)\n",
    "# warranty_types.drop(columns=['time_meses', 'time_dias', 'time_años'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create the 'type_warranty' column based on conditions\n",
    "# conditions = [\n",
    "#     warranty_types['is_warranty_fábrica'],  # 'fábrica' \n",
    "#     warranty_types['is_warranty_vendedor'],   # 'vendedor'\n",
    "# ]\n",
    "\n",
    "# choices = ['fábrica', 'vendedor']\n",
    "\n",
    "# warranty_types['type_warranty'] = np.select(conditions, choices, default=np.nan)\n",
    "# warranty_types.drop(columns=['is_warranty_fábrica', 'is_warranty_vendedor'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warranty_types['warranty_time'] = np.where(warranty_types['Numbers'].isna() | warranty_types['time'].isna(), np.nan,\n",
    "#                                            'warranty_time_' + warranty_types['Numbers'].astype(str) + '_' + warranty_types['time'].astype(str))\n",
    "\n",
    "# warranty_types.drop(columns=['Numbers', 'time'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warranty_types_encoded = pd.get_dummies(warranty_types['type_warranty'])\n",
    "# new_column_names = ['type_warranty_fabrica', 'type_warranty_nan', 'type_warranty_vendedor']\n",
    "# warranty_types_encoded.columns = new_column_names\n",
    "# warranty_types = pd.concat([warranty_types, warranty_types_encoded], axis=1)\n",
    "# warranty_types.drop(columns=['type_warranty', 'Warranty Type Words'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warranty_types['warranty_time'] = warranty_types['warranty_time'].fillna('warranty_time_nan')\n",
    "# warranty_types_encoded = pd.get_dummies(warranty_types['warranty_time'])\n",
    "# warranty_types = pd.concat([warranty_types, warranty_types_encoded], axis=1)\n",
    "# warranty_types.drop(columns=['warranty_time', 'warranty'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comp_data = pd.concat([comp_data, warranty_types], axis=1)\n",
    "# comp_data.drop(columns=['warranty'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_data[['print_server_timestamp', 'date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_data['platform'].str.split('/').str[2].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_data['warranty'].str.split().str[0].str.upper().eq('SIN').value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts_ult = comp_data['full_name'].str.split(' -> ').str[-1].value_counts()\n",
    "# counts_pri = comp_data['full_name'].str.split(' -> ').str[0].value_counts()\n",
    "# tal vez dejar la más amplia como OHE y la otra hacer algo de word2vec?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropeo las columnas.\n",
    "comp_data.drop('benefit', inplace=True, axis=1)\n",
    "# comp_data.drop('user_id', inplace=True, axis=1)\n",
    "comp_data.drop('uid', inplace=True, axis=1)\n",
    "comp_data.drop('main_picture', inplace=True, axis=1)\n",
    "comp_data.drop('category_id', inplace=True, axis=1)\n",
    "comp_data.drop('domain_id', inplace=True, axis=1)\n",
    "comp_data.drop('deal_print_id', inplace=True, axis=1)\n",
    "comp_data.drop('etl_version', inplace=True, axis=1)\n",
    "# comp_data.drop('product_id', inplace=True, axis=1)\n",
    "comp_data.drop('site_id', inplace=True, axis=1)\n",
    "comp_data.drop('item_id', inplace=True, axis=1)\n",
    "comp_data.drop('date', inplace=True, axis=1)\n",
    "comp_data.drop('accepts_mercadopago', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diferenciar desktop, ios, android o mobile(/web/mobile debe ser desde navegador y /mobile/ios desde app)\n",
    "comp_data['platform'] = comp_data['platform'].str.split('/').str[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener primera y última categoría\n",
    "comp_data['category_first'] = comp_data['full_name'].str.split(' -> ').str[0]\n",
    "comp_data['category_last'] = comp_data['full_name'].str.split(' -> ').str[-1]\n",
    "\n",
    "comp_data.drop('full_name', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformo garantía en una columna binaria (True, False, NaN)\n",
    "comp_data['warranty'] = (\n",
    "    ~comp_data['warranty'].str.split().str[0].str.upper().eq('SIN')\n",
    ").where(comp_data['warranty'].notna()).astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo una columna con el descuento (en porcentaje).\n",
    "discount = (((comp_data['original_price'] - comp_data['price']) / comp_data['original_price']) * 100).astype(int)\n",
    "comp_data['discount_%'] = discount\n",
    "\n",
    "comp_data.drop('original_price', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consigo los tags posibles.\n",
    "unique_tags = []\n",
    "for list in comp_data['tags']:\n",
    "    list_split = list[1:len(list)-1].split(', ')\n",
    "    for item in list_split:\n",
    "        if not (item in unique_tags):\n",
    "            unique_tags.append(item)\n",
    "\n",
    "# Separo los tags en columnas de booleanos.\n",
    "for tag in unique_tags:\n",
    "    comp_data[tag] = comp_data['tags'].apply(lambda x: tag in x)\n",
    "\n",
    "comp_data.drop('tags', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacer algo inteligente con la date.\n",
    "comp_data['date'] = pd.to_datetime(comp_data['print_server_timestamp'])\n",
    "comp_data['month'] = comp_data['date'].dt.month\n",
    "comp_data['day'] = comp_data['date'].dt.day\n",
    "comp_data['day_of_week'] = comp_data['date'].dt.dayofweek\n",
    "comp_data['hour'] = comp_data['date'].dt.hour\n",
    "comp_data['minute'] = comp_data['date'].dt.minute\n",
    "comp_data['second'] = comp_data['date'].dt.second\n",
    "\n",
    "comp_data.drop('date', inplace=True, axis=1)\n",
    "comp_data.drop('print_server_timestamp', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "# comp_data = comp_data.loc[:,~comp_data.columns.str.startswith('warranty')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esto puede generar data leakage pero como al final entrenamos con todo lo pruebo y valido con Kaggle\n",
    "\n",
    "user_counting = comp_data['user_id'].value_counts()\n",
    "\n",
    "uid_ratio = comp_data[['user_id', 'conversion']]\n",
    "pid_ratio = comp_data[['product_id', 'conversion']]\n",
    "\n",
    "# Calculate conversion frequency\n",
    "conversion_frequency = pid_ratio.groupby('product_id')['conversion'].mean().reset_index()\n",
    "\n",
    "# Calculate count of each unique product_id\n",
    "product_id_count = pid_ratio['product_id'].value_counts().reset_index()\n",
    "product_id_count.columns = ['product_id', 'pid_count']\n",
    "\n",
    "# Merge the two DataFrames\n",
    "result = pd.merge(conversion_frequency, product_id_count, on='product_id')\n",
    "\n",
    "# Rename the columns if needed\n",
    "result.rename(columns={'conversion': 'conv_freq_pid'}, inplace=True)\n",
    "\n",
    "comp_data = pd.merge(comp_data, result, on='product_id')\n",
    "\n",
    "# Calculate conversion frequency\n",
    "conversion_frequency_user = uid_ratio.groupby('user_id')['conversion'].mean().reset_index()\n",
    "\n",
    "# Calculate count of each unique product_id\n",
    "user_id_count = uid_ratio['user_id'].value_counts().reset_index()\n",
    "user_id_count.columns = ['user_id', 'uid_count']\n",
    "\n",
    "# Merge the two DataFrames\n",
    "result_uid = pd.merge(conversion_frequency_user, user_id_count, on='user_id')\n",
    "\n",
    "# Rename the columns if needed\n",
    "result_uid.rename(columns={'conversion': 'conv_freq_uid'}, inplace=True)\n",
    "\n",
    "comp_data = pd.merge(comp_data, result_uid, on='user_id')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(raw_text):\n",
    "    \"\"\"\n",
    "    Tokeniza y preprocesa un texto.\n",
    "\n",
    "    Args:\n",
    "        raw_text (str): Texto sin procesar.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de oraciones, donde cada oración es una lista de palabras.\n",
    "    \"\"\"\n",
    "    sentences = sent_tokenize(raw_text)\n",
    "    sentences = [word_tokenize(e) for e in sentences]\n",
    "    sentences = [[e2 for e2 in e1 if re.compile(\"[A-Za-z]\").search(e2[0])] for e1 in sentences]\n",
    "    sentences = [[e2.lower() for e2 in e1] for e1 in sentences]\n",
    "    return(sentences)\n",
    "\n",
    "def average_vectors(title_tokens, model, stopwords=None):\n",
    "    \"\"\"\n",
    "    Calcula el vector promedio de un conjunto de tokens utilizando un modelo Word2Vec.\n",
    "\n",
    "    Args:\n",
    "        title_tokens (list): Lista de tokens.\n",
    "        model (gensim.models.Word2Vec): Modelo Word2Vec.\n",
    "        stopwords (set, optional): Conjunto de palabras stopwords. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Vector promedio.\n",
    "    \"\"\"\n",
    "    title_tokens = [e2 for e1 in title_tokens for e2 in e1]\n",
    "    title_tokens = [e for e in title_tokens if e in model.wv]\n",
    "    if stopwords is not None:\n",
    "        title_tokens = [e for e in title_tokens if e not in stopwords]\n",
    "    if len(title_tokens) == 0:\n",
    "        output = np.zeros(model.wv.vector_size)\n",
    "    else:\n",
    "        output = np.array([model.wv.get_vector(e) for e in title_tokens]).mean(0)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Para title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_data['title_tokens'] = comp_data['title'].map(tokenizer)\n",
    "\n",
    "STOP_WORDS_SP = set(stopwords.words('spanish'))\n",
    "\n",
    "# Creación del modelo Word2Vec\n",
    "w2v_title = gensim.models.Word2Vec(vector_size=100,\n",
    "                                   window=3,\n",
    "                                   min_count=5,\n",
    "                                   negative=15,\n",
    "                                   sample=0.01,\n",
    "                                   workers=8,\n",
    "                                   sg=1)\n",
    "\n",
    "# Creación del vocabulario a partir del corpus\n",
    "w2v_title.build_vocab([e2 for e1 in comp_data['title_tokens'].values for e2 in e1], \n",
    "                       progress_per=10000)\n",
    "\n",
    "# Entrenamiento del modelo Word2Vec\n",
    "w2v_title.train([e2 for e1 in comp_data['title_tokens'].values for e2 in e1],\n",
    "                total_examples=w2v_title.corpus_count,\n",
    "                epochs=30, report_delay=1)\n",
    "\n",
    "title_embs = comp_data['title_tokens'].map(lambda x: average_vectors(x, w2v_title, STOP_WORDS_SP))\n",
    "embedding_title_columns = pd.DataFrame(title_embs.tolist(), columns=[f'title_emb_{i}' for i in range(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "evol_variabilidad = []\n",
    "for k in range(10, 50):\n",
    "    kmeans = KMeans(n_clusters=k, max_iter=30, n_init=20)\n",
    "    kmeans.fit(embedding_title_columns)\n",
    "    evol_variabilidad.append({\"k\": k, \"var\": kmeans.inertia_})\n",
    "\n",
    "evol_variabilidad = pd.DataFrame(evol_variabilidad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(evol_variabilidad[\"k\"], evol_variabilidad[\"var\"], marker=\"o\")\n",
    "plt.xlabel(\"# Clusters\")\n",
    "plt.ylabel(\"tot.withinss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_data.drop('title_tokens', inplace=True, axis=1)\n",
    "comp_data.drop('title', inplace=True, axis=1)\n",
    "comp_data = pd.concat([comp_data, embedding_title_columns], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_encode = ['category_first', 'listing_type_id', 'logistic_type', 'platform', 'category_last']\n",
    "comp_data_encoded = pd.get_dummies(comp_data[cols_to_encode])\n",
    "comp_data = pd.concat([comp_data, comp_data_encoded], axis=1)\n",
    "comp_data.drop(columns=cols_to_encode, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Antes de empezar el entrenamiento del modelo, paso a int las columnas de booleano. Lo hago así para no tener problemas con los NaNs\n",
    "comp_data.replace({False: 0, True: 1}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_data.to_csv(\"data/new_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC-ROC validación: 0.89293 --> xgboost_model_w2v_2709\n",
    "# AUC-ROC validación: 0.89365 --> xgboost_model_w2v100_war_2709"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "td6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
